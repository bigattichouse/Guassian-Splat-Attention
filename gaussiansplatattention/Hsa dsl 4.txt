# HSA-DSL: Domain Specific Language for Hierarchical SplatNet Attention

# HIERARCHY DEFINITION
hierarchy HSA {
  levels = [Token, Phrase, Section, Document]
  init_splats_per_level = [100, 50, 20, 5]
  level_weights = [0.4, 0.3, 0.2, 0.1]
}

# INITIALIZATION METHODS
initialization {
  # Advanced clustering for splat center initialization
  clustering {
    methods = [spectral, pca, random]
    default_method = spectral
    
    # Spectral clustering parameters
    spectral {
      affinity = nearest_neighbors
      n_neighbors = min(10, data_points / 2)
      silhouette_optimization = true
    }
    
    # PCA dimension reduction before clustering
    pca {
      enabled = true
      max_components = 10
      variance_threshold = 0.9
    }
    
    # Optimal cluster determination
    optimal_clusters {
      max_clusters = 10
      metric = silhouette_score
      min_score_threshold = 0.3
    }
    
    # Initialize splat centers method
    initialize_splat_centers(level) {
      # Generate initial data points
      data = sample_data_points(level)
      
      # Apply dimension reduction if needed
      if pca.enabled and data.dimensions > 1 {
        reduced_data = apply_pca(data, pca.max_components)
      } else {
        reduced_data = data
      }
      
      # Determine optimal number of clusters
      if optimal_clusters.metric == silhouette_score {
        n_clusters = compute_optimal_clusters(
          reduced_data, 
          optimal_clusters.max_clusters, 
          optimal_clusters.min_score_threshold
        )
      } else {
        n_clusters = init_splats_per_level[level]
      }
      
      # Apply clustering algorithm
      if default_method == spectral {
        centers = apply_spectral_clustering(reduced_data, n_clusters, spectral.affinity)
      } else if default_method == random {
        centers = sample_random_centers(reduced_data, n_clusters)
      }
      
      return centers
    }
  }
}

# SPLAT DEFINITION
splat {
  # Core properties
  position: learnable vector(dim=model_dim, init=clustering.initialize_splat_centers)
  covariance: learnable matrix(dim=model_dim, positive_definite=true)
  tensor_field: learnable tensor(dim=model_dim)
  amplitude: learnable scalar(positive=true)
  level: fixed enum(hierarchy.levels)
  
  # Connection importance (from Python implementation)
  connection_importance: learnable vector(dim=topk_connections, positive=true)
  
  # Temperature parameter for soft selection
  temperature: learnable scalar(positive=true, init=1.0)
  
  # Information-theoretic properties
  information_contribution: computed scalar
  relative_entropy: computed scalar
  attention_entropy: computed scalar
  
  # Relationships
  parent: optional ref(splat at level+1)
  children: set(ref(splat at level-1))
  
  # Methods
  distance(i, j) = mahalanobis((tokens[i] - tokens[j]) - position, covariance)
  field_effect(i, j) = exp(gamma * (j-i)' * tensor_field * (j-i))
  
  # Soft top-k selection with temperature
  soft_topk(scores) = softmax(scores / temperature)
  
  # Attention contribution with importance weighting
  attention(i, j) = amplitude * exp(-distance(i, j)^2) * field_effect(i, j)
  weighted_attention(i, j, k) = attention(i, j) * connection_importance[k]
  
  # Information-theoretic methods
  compute_attention_entropy(attention_distribution) {
    # Compute Shannon entropy of attention distribution
    return -sum(p * log(p) for p in attention_distribution where p > 0)
  }
  
  compute_information_contribution(tokens, attention_with_splat, attention_without_splat) {
    # Compute KL divergence between attention with and without this splat
    kl_div = sum(attention_with_splat[i, j] * log(attention_with_splat[i, j] / attention_without_splat[i, j])
              for i, j in tokens.indices where attention_with_splat[i, j] > 0)
    
    # Normalize by number of token pairs
    return kl_div / (tokens.length * tokens.length)
  }
  
  compute_mutual_information(tokens, attention_distribution) {
    # Compute mutual information between tokens based on attention
    joint_prob = attention_distribution
    marginal_i = sum(joint_prob, axis=1)
    marginal_j = sum(joint_prob, axis=0)
    
    mi = sum(joint_prob[i, j] * log(joint_prob[i, j] / (marginal_i[i] * marginal_j[j]))
          for i, j in tokens.indices where joint_prob[i, j] > 0)
    
    # Normalize by maximal possible MI for the distribution
    max_entropy = min(compute_attention_entropy(marginal_i), compute_attention_entropy(marginal_j))
    return mi / max_entropy if max_entropy > 0 else 0
  }
}

# ATTENTION MECHANISM
attention HSA {
  # Inputs and outputs
  inputs: tokens[sequence_length, model_dim]
  outputs: attention_matrix[sequence_length, sequence_length]
  
  # Sparse attention parameters
  sparse_topk = 64
  
  # Hybrid attention mechanism
  use_hybrid_attention = true
  structured_sparse_attention_weight = learnable scalar(init=0.3, range=[0, 1])
  
  # Calculate attention
  compute(tokens) {
    # Initialize attention matrices
    splat_attention_matrix = zeros(sequence_length, sequence_length)
    
    if use_hybrid_attention {
      # Compute structured sparse attention (e.g., local window + global tokens)
      sparse_attention_matrix = compute_structured_sparse_attention(tokens)
      
      # Learn selection mask for hybrid attention
      selection_mask = compute_attention_selection_mask(tokens)
    }
    
    # Compute hierarchical splat attention
    for each level in hierarchy.levels {
      # Get all splats at this level
      level_splats = splats.filter(splat => splat.level == level)
      
      # Compute raw attention scores
      raw_scores = {}
      for each splat in level_splats {
        for each i, j in tokens.indices {
          raw_scores[i, j] += splat.attention(i, j)
        }
      }
      
      # Apply soft top-k selection with temperature
      for each i in tokens.indices {
        topk_indices = top_k_indices(raw_scores[i, :], k=sparse_topk)
        for k, j in enumerate(topk_indices) {
          # Apply connection importance weighting
          level_contrib[i, j] += weighted_sum([
            splat.weighted_attention(i, j, k) 
            for splat in level_splats
          ])
        }
      }
    }
    
    # Combine hierarchical contributions with level weights
    splat_attention_matrix = weighted_sum(level_contrib, hierarchy.level_weights)
    
    # Apply hybrid attention if enabled
    if use_hybrid_attention {
      final_attention = sparse_attention_matrix * selection_mask + 
                        splat_attention_matrix * (1 - selection_mask)
      return final_attention
    } else {
      return splat_attention_matrix
    }
  }
  
  # Compute structured sparse attention (local + global patterns)
  compute_structured_sparse_attention(tokens) {
    # Initialize with local window attention
    local_window_size = 128
    sparse_attention = compute_local_window_attention(tokens, local_window_size)
    
    # Add global token attention
    global_tokens = identify_global_tokens(tokens)
    sparse_attention = add_global_token_attention(sparse_attention, global_tokens)
    
    return sparse_attention
  }
  
  # Compute selection mask for hybrid attention
  compute_attention_selection_mask(tokens) {
    # Learn a binary mask that selects between sparse and splat attention
    # based on token relationships and positions
    return sigmoid(selection_network(tokens))
  }
  
  # Compute information-theoretic metrics for the attention distribution
  analyze_information_metrics(attention_matrix) {
    metrics = {
      global_entropy: compute_global_entropy(attention_matrix),
      token_entropy: compute_per_token_entropy(attention_matrix),
      mutual_information: compute_global_mutual_information(attention_matrix),
      information_flow: analyze_hierarchical_information_flow()
    }
    
    return metrics
  }
}

# INFORMATION THEORY FOUNDATION
information_theory {
  # Core information-theoretic mechanisms
  metrics {
    # Entropy-based metrics
    attention_entropy(distribution) = -sum(p * log(p) for p in distribution where p > 0)
    cross_entropy(p, q) = -sum(p * log(q) for p, q where p > 0)
    kl_divergence(p, q) = sum(p * log(p/q) for p, q where p > 0)
    js_divergence(p, q) = 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m) where m = 0.5 * (p + q)
    
    # Mutual information
    mutual_information(joint, marginal_x, marginal_y) = 
      sum(joint[x,y] * log(joint[x,y] / (marginal_x[x] * marginal_y[y])) 
          for all x, y where joint[x,y] > 0)
    
    # Normalized mutual information (for comparing across different sequence lengths)
    normalized_mi(mi, entropies) = mi / min(entropies)
    
    # Information gain metrics
    expected_information_gain(splat, tokens) {
      # Compute attention with and without this splat
      attention_with = compute_attention_with_splat(splat, tokens)
      attention_without = compute_attention_without_splat(splat, tokens)
      
      # Compute KL divergence between the two distributions
      return kl_divergence(attention_with, attention_without)
    }
  }
  
  # Relative entropy minimization
  relative_entropy_preservation {
    # When making structural changes, minimize information loss
    max_allowable_kl = 0.01  # Maximum KL divergence allowed after structural change
    
    # Compute information loss from a structural change
    measure_information_loss(attention_before, attention_after) {
      return kl_divergence(attention_before, attention_after)
    }
    
    # Validate structural change based on information preservation
    is_acceptable_change(attention_before, attention_after) {
      return measure_information_loss(attention_before, attention_after) <= max_allowable_kl
    }
  }
  
  # Mutual information maximization
  mutual_information_maximization {
    # Maximize mutual information between relevant token pairs
    objective = "maximize MI between tokens while maintaining sparsity"
    
    # Add MI-based regularization term to the loss function
    regularization_weight = 0.1
    
    # MI-based loss component
    compute_mi_loss(attention) {
      # Extract joint and marginal distributions from attention
      joint_dist = normalize(attention)
      row_marginals = sum(joint_dist, axis=1)
      col_marginals = sum(joint_dist, axis=0)
      
      # Compute mutual information
      mi = mutual_information(joint_dist, row_marginals, col_marginals)
      
      # Normalize by maximum possible MI
      max_entropy = min(attention_entropy(row_marginals), attention_entropy(col_marginals))
      normalized_mi = mi / max_entropy if max_entropy > 0 else 0
      
      # We want to maximize MI, so return negative for loss minimization
      return -normalized_mi * regularization_weight
    }
  }
  
  # Scale-invariant information processing
  scale_invariance {
    # Ensure consistent behavior across different sequence lengths
    sequence_length_normalization = true
    
    # Information density metrics adjusted for sequence length
    normalized_information_density(info, seq_len) = info / log(seq_len)
    
    # Scale-adaptive information thresholds
    adaptive_threshold(base_threshold, seq_len) = base_threshold * log(seq_len) / log(1024)
  }
  
  # Hierarchical information flow analysis
  hierarchical_information_flow {
    # Analyze information propagation across hierarchical levels
    
    # Information transfer metrics between levels
    level_transfer(level_a, level_b) {
      # Measure how much information from level_a influences level_b
      attention_a = compute_level_attention(level_a)
      attention_b = compute_level_attention(level_b)
      
      # Compute conditional entropy reduction
      return entropy_reduction(attention_b | attention_a)
    }
    
    # Information bottleneck analysis
    information_bottleneck(level) {
      # Compute if a level is constraining information flow
      mi_in = level_transfer(level-1, level)
      mi_out = level_transfer(level, level+1)
      
      return mi_out / mi_in if mi_in > 0 else 1.0
    }
    
    # Ensure balanced information flow across the hierarchy
    optimize_hierarchical_flow() {
      # Adjust level weights to maximize end-to-end information transfer
      # while maintaining computational efficiency
      bottlenecks = [information_bottleneck(level) for level in hierarchy.levels]
      return adjust_level_weights_for_balanced_flow(bottlenecks)
    }
  }
}

# ADAPTIVE MECHANISMS
adaptation {
  # Information-theoretic decision making for adaptation
  adaptation_decisions {
    # Use information gain to guide adaptation decisions
    minimum_information_gain = 0.001  # Minimum gain required for adaptation
    
    # Assess if adaptation would be beneficial
    should_adapt(current_attention, projected_attention) {
      info_gain = information_theory.metrics.expected_information_gain(
        current_attention, projected_attention
      )
      
      # Apply sequence length normalization if enabled
      if information_theory.scale_invariance.sequence_length_normalization {
        info_gain = information_theory.scale_invariance.normalized_information_density(
          info_gain, sequence_length
        )
      }
      
      return info_gain > minimum_information_gain
    }
  }
  
  # Mitosis (Splat Division)
  mitosis {
    trigger when {
      (splat.error_contribution > tau_m(splat.level)) and
      (splat.attention_entropy > var_threshold) and
      (information_theory.metrics.expected_information_gain(splat, tokens) > cost_threshold) and
      adaptation_decisions.should_adapt(current_attention, projected_post_mitosis_attention)
    }
    
    execute {
      # Create two children with perturbed parameters
      child1 = splat.clone()
      child2 = splat.clone()
      
      # Modify positions slightly along the direction of maximum information gradient
      info_gradient = compute_information_gradient(splat)
      child1.position = splat.position + perturb(scale=0.1, direction=info_gradient)
      child2.position = splat.position - perturb(scale=0.1, direction=info_gradient)
      
      # Reduce covariance (more focused)
      child1.covariance = splat.covariance * 0.8
      child2.covariance = splat.covariance * 0.8
      
      # Divide amplitude, preserving energy
      child1.amplitude = splat.amplitude * 0.5
      child2.amplitude = splat.amplitude * 0.5
      
      # Slightly modify tensor fields for diversity
      child1.tensor_field = splat.tensor_field + perturb(scale=0.05)
      child2.tensor_field = splat.tensor_field - perturb(scale=0.05)
      
      # Transfer connection importance with small perturbations
      child1.connection_importance = splat.connection_importance * (1 + perturb(scale=0.05))
      child2.connection_importance = splat.connection_importance * (1 + perturb(scale=0.05))
      
      # Slightly diversify temperature
      child1.temperature = splat.temperature * (1 + perturb(scale=0.02))
      child2.temperature = splat.temperature * (1 + perturb(scale=0.02))
      
      # Verify information preservation
      projected_attention = compute_projected_attention([child1, child2], tokens)
      if information_theory.relative_entropy_preservation.is_acceptable_change(
          current_attention, projected_attention) {
        # Replace parent with children
        replace(splat, [child1, child2])
      } else {
        # Adjust parameters to reduce information loss
        refine_parameters_for_information_preservation([child1, child2])
        replace(splat, [child1, child2])
      }
    }
  }
  
  # Death (Pruning)
  death {
    trigger when {
      (splat.activation < tau_d(splat.level) for consecutive_batches > 5) or
      (splat.information_contribution < min_contribution) or
      (exists other_splat where similarity(splat, other_splat) > sim_threshold)
    }
    
    execute {
      # Calculate information loss before removal
      attention_before = compute_current_attention()
      projected_attention = compute_attention_without_splat(splat)
      
      info_loss = information_theory.relative_entropy_preservation.measure_information_loss(
        attention_before, projected_attention
      )
      
      # If information loss is acceptable
      if info_loss < information_theory.relative_entropy_preservation.max_allowable_kl {
        # Find nearby splats to redistribute energy
        neighbors = find_nearest_splats(splat, k=3)
        
        # Determine optimal energy redistribution to minimize information loss
        redistribution_weights = compute_optimal_redistribution(
          splat, neighbors, attention_before
        )
        
        # Redistribute energy and parameters
        for i, neighbor in enumerate(neighbors) {
          weight = redistribution_weights[i]
          neighbor.amplitude += splat.amplitude * weight
          neighbor.connection_importance += splat.connection_importance * weight
          
          # Fine-tune neighbor parameters to minimize information loss
          fine_tune_neighbor_for_information_preservation(
            neighbor, splat, weight, attention_before
          )
        }
        
        remove(splat)
      } else {
        # If information loss too high, just fade amplitude gradually
        splat.amplitude *= fade_factor
      }
    }
  }
  
  # Cross-level interactions
  cross_level {
    # Top-down refinement: parent influences children
    top_down(parent_splat) {
      for child in parent_splat.children {
        # Calculate information transfer from parent to child
        info_transfer = information_theory.hierarchical_information_flow.level_transfer(
          parent_splat.level, child.level
        )
        
        # Adjust child based on parent patterns, weighted by information transfer
        alpha_top_adjusted = alpha_top * info_transfer
        child.tensor_field += alpha_top_adjusted * (parent_splat.tensor_field - child.tensor_field)
        child.connection_importance += alpha_top_adjusted * (parent_splat.connection_importance - child.connection_importance)
      }
    }
    
    # Bottom-up aggregation: children influence parent
    bottom_up(parent_splat) {
      if parent_splat.children.size > 0 {
        # Calculate information relevance of each child
        info_relevance = [child.information_contribution for child in parent_splat.children]
        normalized_relevance = normalize(info_relevance)
        
        # Weighted average of children's parameters based on information relevance
        weighted_field = weighted_sum(
          [child.tensor_field for child in parent_splat.children],
          normalized_relevance
        )
        parent_splat.tensor_field += alpha_bottom * (weighted_field - parent_splat.tensor_field)
        
        weighted_importance = weighted_sum(
          [child.connection_importance for child in parent_splat.children],
          normalized_relevance
        )
        parent_splat.connection_importance += alpha_bottom * (weighted_importance - parent_splat.connection_importance)
      }
    }
    
    # Temperature adaptation based on level relationships and information metrics
    adapt_temperature {
      # Lower-level splats should have sharper attention (lower temperature)
      for each level in hierarchy.levels {
        # Compute information entropy at this level
        level_entropy = average(splat.attention_entropy for splat in splats where splat.level == level)
        
        # Adjust target temperature based on entropy
        entropy_factor = soft_clamp(level_entropy / max_entropy, 0.5, 2.0)
        target_temp = base_temperature * (temp_level_factor ^ level_index) * entropy_factor
        
        for splat in splats where splat.level == level {
          splat.temperature += temperature_lr * (target_temp - splat.temperature)
        }
      }
    }
  }
}

# TRAINING AND OPTIMIZATION
training {
  phases = [
    {name: "structure_learning", epochs: 5, adaptation_rate: 0.1},
    {name: "pattern_specialization", epochs: 10, adaptation_rate: 0.05},
    {name: "information_optimization", epochs: 7, adaptation_rate: 0.03},
    {name: "regularization", epochs: 5, adaptation_rate: 0.02},
    {name: "stabilization", epochs: 3, adaptation_rate: 0.01}
  ]
  
  # Information-theoretic loss components
  loss_components {
    task_loss: 1.0,                      # Primary task loss (e.g., language modeling)
    mutual_information_loss: 0.1,        # Promote high mutual information in relevant attention
    relative_entropy_preservation: 0.2,  # Preserve information during structural changes
    structural_complexity: 0.05,         # Penalize excessive splat count
    hierarchical_balance: 0.05           # Ensure balanced information flow across levels
  }
  
  # Annealing schedules for information-theoretic parameters
  annealing {
    # Mitosis threshold annealing
    tau_m(t, level) = base_tau_m(level) * (1 + alpha_m * t/T)
    
    # Pruning threshold annealing
    tau_d(t, level) = base_tau_d(level) * (1 - alpha_d * t/T)
    
    # Information gain threshold annealing
    min_info_gain(t) = base_min_info_gain * (1 + beta_i * t/T)
    
    # KL divergence tolerance annealing
    max_kl_divergence(t) = base_max_kl * (1 - gamma_kl * t/T)
    
    # Mutual information weight annealing
    mi_weight(t) = base_mi_weight * (1 - delta_mi * t/T)
  }
  
  # Hyperparameters for adaptation
  tau_m(level) = base_tau_m * (level_factor ^ level_index)  # Mitosis threshold
  tau_d(level) = base_tau_d * (level_factor ^ level_index)  # Death threshold
  
  # Temperature parameters
  base_temperature = 1.0
  temp_level_factor = 1.2  # Higher levels get higher temperature (softer attention)
  temperature_lr = 0.01    # Learning rate for temperature adaptation
  
  # Complexity regulation with information-theoretic guidance
  complexity_regulation {
    target_range = [min: total_tokens * 0.05, max: total_tokens * 0.1]
    
    # Information-aware complexity adjustment
    adjust_complexity() {
      current_splats = total_splats
      
      # If too many splats, increase death rate with information preservation
      if current_splats > target_range.max {
        # Identify least informative splats
        splats_by_info = sort_splats_by_information_contribution(ascending=true)
        
        # Increase death rate proportionally to excess
        excess_ratio = (current_splats - target_range.max) / target_range.max
        increase_death_rate(factor=1.0 + excess_ratio)
        
        # Prioritize pruning least informative splats
        prioritize_for_pruning(splats_by_info.slice(0, int(current_splats * excess_ratio)))
      }
      
      # If too few splats, increase mitosis rate with information guidance
      if current_splats < target_range.min {
        # Identify most informative splats for division
        splats_by_strain = sort_splats_by_strain_metric(descending=true)
        
        # Increase mitosis rate proportionally to deficit
        deficit_ratio = (target_range.min - current_splats) / target_range.min
        increase_mitosis_rate(factor=1.0 + deficit_ratio)
        
        # Prioritize division of high-strain splats
        prioritize_for_mitosis(splats_by_strain.slice(0, int(current_splats * deficit_ratio)))
      }
    }
  }
}

# MODEL INTEGRATION
model {
  # How HSA integrates with transformer architecture
  integration {
    replace_standard_attention(HSA)
    adapt_gradients_for_structural_learning()
    
    # Information-guided sparse computation
    if sequence_length > long_threshold {
      enable_sparse_computation()
      prioritize_higher_level_splats()
      enable_information_guided_sparsity()
    }
  }
  
  # Connection to attention projections
  projections {
    query_proj = create_projection_block(dim, dropout)
    key_proj = create_projection_block(dim, dropout)
    value_proj = create_projection_block(dim, dropout)
    out_proj = create_projection_block(dim, dropout)
    
    # Create regularized projection block with residual connections
    create_projection_block(dim, dropout) {
      return Sequential([
        Linear(dim, dim),
        LayerNorm(dim),
        GELU(),
        Dropout(dropout),
        Linear(dim, dim)
      ])
    }
  }
  
  # Information-theoretic monitoring during training
  monitoring {
    # Track information-theoretic metrics during training
    track_metrics = [
      "attention_entropy",
      "mutual_information",
      "hierarchical_information_flow",
      "information_contribution_per_splat",
      "kl_divergence_after_adaptation"
    ]
    
    # Information-based early stopping
    early_stopping {
      monitor = "validation_mutual_information"
      patience = 5
      min_delta = 0.001
      mode = "max"  # Higher mutual information is better
    }
  }
}

# COMPUTATIONAL OPTIMIZATIONS
optimizations {
  # Information-guided sparse computation
  sparse {
    # Only compute attention for relevant splats
    relevance_threshold = 0.01
    max_splats_per_token = 50
    
    # Information-guided sparsity
    information_guided_sparsity {
      enabled = true
      min_information_contribution = 0.001
      
      # Prioritize splats by information contribution
      splat_selection_strategy = "max_information"
      
      # Adapt sparsity based on sequence complexity
      adaptive_sparsity = true
      complexity_metric = "token_entropy"
    }
    
    # Parallel processing
    parallelize_across_levels = true
    parallelize_across_splats = true
  }
  
  # Inference optimizations
  inference {
    cache_splat_positions = true
    use_approximate_distance = true
    enable_early_stopping = true
    
    # Information-aware pruning during inference
    information_aware_pruning {
      enabled = true
      max_information_loss = 0.005
      prune_strategy = "least_informative_first"
    }
    
    # Temperature annealing during inference
    anneal_temperature = true
    final_temperature = 0.5
  }

# Long context optimization
long_context {
  # Special handling for extremely long sequences
  million_token_optimizations {
    enable_token_dropping = true
    token_importance_threshold = adaptive_threshold(0.001, sequence_length)
    hierarchical_caching = true
    attention_checkpoint_interval = 10000  # Cache attention results every N tokens
  }
}
}

# VISUALIZATION AND ANALYSIS
visualization {
  # Information-theoretic visualizations
  info_visualizations {
    # Mutual information heatmap
    mutual_information_heatmap {
      colormap = "viridis"
      normalize = true
      cluster_by_similarity = true
    }
    
    # Entropy reduction visualization
    entropy_reduction_flow {
      show_hierarchical_levels = true
      show_information_bottlenecks = true
      highlight_critical_paths = true
    }
    
    # Information contribution per splat
    splat_information_contribution {
      size_by_contribution = true
      color_by_level = true
      show_connections = true
      min_connection_strength = 0.1
    }
  }
  
  # Hierarchical splat visualization
  hierarchical_visualization {
    # Visual encoding per level
    level_encoding = [
      {color: "blue", opacity: 0.8, border: "solid"},
      {color: "green", opacity: 0.7, border: "dashed"},
      {color: "orange", opacity: 0.6, border: "dotted"},
      {color: "red", opacity: 0.5, border: "none"}
    ]
    
    # Tensor field visualization
    show_tensor_fields = true
    tensor_field_representation = "ellipsoid"  # or "vector_field", "heatmap"
    
    # Cross-level connections
    show_cross_level_connections = true
    connection_threshold = 0.1
    connection_color_by = "information_transfer"  # or "strength", "direction"
  }
  
  # Attention analysis tools
  attention_analysis {
    # Multi-scale attention graphs
    multi_scale_attention {
      levels_to_show = "all"  # or specific levels
      token_aggregation = "mean"  # how to aggregate token-level attention
      highlight_strongest_connections = true
      max_connections_per_token = 10
    }
    
    # Information flow diagrams
    information_flow {
      show_direction = true
      quantify_flow = true
      aggregate_by = "semantic_units"  # or "tokens", "splats"
    }
  }
}