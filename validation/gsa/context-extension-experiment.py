#!/usr/bin/env python3
"""
GSA Context Extension Test

Simple validation script to test if GSA can extend context length
for TinyStories-Instruct-33M model.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
import time
from dataclasses import dataclass
from typing import Optional, Tuple

# GSA Configuration
@dataclass
class GSAConfig:
    dim: int = 256
    n_heads: int = 8
    n_splats_per_head: int = 12
    movement_scale: float = 0.08
    pruning_threshold: float = 0.02
    temperature_init: float = 1.0
    scale_init: float = 0.5
    
    @property
    def head_dim(self) -> int:
        return self.dim // self.n_heads

# Simplified GSA Implementation
class GSAMechanism(nn.Module):
    def __init__(self, config: GSAConfig):
        super().__init__()
        self.config = config
        
        # Core splat parameters
        self.splat_centers = nn.Parameter(torch.randn(config.n_heads, config.n_splats_per_head, config.head_dim) * 0.2)
        self.splat_deltas = nn.Parameter(torch.zeros(config.n_heads, config.n_splats_per_head, config.head_dim))
        self.splat_log_scales = nn.Parameter(torch.randn(config.n_heads, config.n_splats_per_head) * 0.2 + np.log(config.scale_init))
        self.splat_log_amplitudes = nn.Parameter(torch.randn(config.n_heads, config.n_splats_per_head) * 0.1 - 0.5)
        
        # Control parameters
        self.movement_scale = nn.Parameter(torch.tensor(config.movement_scale))
        self.temperature = nn.Parameter(torch.tensor(config.temperature_init))
        
        # Projections
        self.qkv_proj = nn.Linear(config.dim, 3 * config.dim, bias=False)
        self.out_proj = nn.Linear(config.dim, config.dim, bias=False)
        
        # Initialize
        nn.init.xavier_uniform_(self.qkv_proj.weight, gain=0.5)
        nn.init.xavier_uniform_(self.out_proj.weight, gain=0.5)
    
    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):
        B, T, D = x.shape
        H, S = self.config.n_heads, self.config.n_splats_per_head
        head_dim = self.config.head_dim
        
        # Project to Q, K, V
        qkv = self.qkv_proj(x).reshape(B, T, 3, H, head_dim)
        q, k, v = qkv.unbind(2)
        
        # Get splat parameters
        centers = self.splat_centers + self.splat_deltas * torch.sigmoid(self.movement_scale) * 0.2
        scales = torch.exp(self.splat_log_scales).clamp(min=0.01, max=2.0)
        amplitudes = torch.exp(self.splat_log_amplitudes).clamp(min=1e-6, max=10.0)
        
        # Compute attention
        attention_logits = torch.zeros(B, T, T, H, device=x.device)
        
        for h in range(H):
            # Distances from tokens to splats
            q_dists = torch.sum((q[:, :, h].unsqueeze(2) - centers[h]) ** 2, dim=-1)  # [B, T, S]
            k_dists = torch.sum((k[:, :, h].unsqueeze(2) - centers[h]) ** 2, dim=-1)  # [B, T, S]
            
            # Gaussian weights
            q_weights = torch.exp(-0.5 * q_dists / (scales[h] ** 2 + 1e-8))
            k_weights = torch.exp(-0.5 * k_dists / (scales[h] ** 2 + 1e-8))
            
            # Attention through splats
            attention_logits[:, :, :, h] = torch.einsum('bis,bjs,s->bij', q_weights, k_weights, amplitudes[h])
        
        # Apply temperature and normalize
        attention = F.softmax(attention_logits / self.temperature.clamp(min=0.1, max=10.0), dim=2)
        
        # Apply to values
        output = torch.einsum('btjh,bjhd->bthd', attention, v)
        output = output.reshape(B, T, D)
        return self.out_proj(output)

class GSALayer(nn.Module):
    def __init__(self, config: GSAConfig):
        super().__init__()
        self.attention = GSAMechanism(config)
        self.norm = nn.LayerNorm(config.dim)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, hidden_states, attention_mask=None, layer_past=None, 
                head_mask=None, use_cache=False, output_attentions=False, **kwargs):
        # GSA forward pass
        normed = self.norm(hidden_states)
        attn_out = self.attention(normed, attention_mask)
        output = hidden_states + self.dropout(attn_out)
        
        # Return in format expected by transformer models
        outputs = (output,)
        
        if use_cache:
            outputs = outputs + (None,)  # No past key values for GSA
        
        if output_attentions:
            outputs = outputs + (None,)  # No attention weights returned
            
        return outputs

class GSAModelWrapper(nn.Module):
    def __init__(self, base_model, gsa_config, extend_positions=True):
        super().__init__()
        self.model = base_model
        self.gsa_config = gsa_config
        self.original_max_pos = getattr(base_model.config, 'max_position_embeddings', 2048)
        
        if extend_positions:
            self._extend_position_embeddings()
        
        self._replace_attention()
        
    def _extend_position_embeddings(self):
        """Extend position embeddings to support longer contexts"""
        new_max_pos = 16384  # Extend to 16K tokens
        
        print(f"Extending position embeddings: {self.original_max_pos} ‚Üí {new_max_pos}")
        
        # Get current position embeddings
        if hasattr(self.model.transformer, 'wpe'):
            # GPT-2 style
            old_pos_emb = self.model.transformer.wpe.weight.data
            embed_dim = old_pos_emb.shape[1]
            
            # Create new larger embedding
            new_pos_emb = nn.Embedding(new_max_pos, embed_dim)
            
            # Copy old embeddings
            new_pos_emb.weight.data[:self.original_max_pos] = old_pos_emb
            
            # Initialize new positions by repeating/interpolating
            for i in range(self.original_max_pos, new_max_pos):
                # Use cyclic repetition of position embeddings
                old_idx = i % self.original_max_pos
                new_pos_emb.weight.data[i] = old_pos_emb[old_idx] * 0.95  # Slight decay
            
            self.model.transformer.wpe = new_pos_emb
            self.model.config.max_position_embeddings = new_max_pos
            
            print(f"  ‚úÖ Extended position embeddings successfully")
            
        else:
            print("  ‚ö†Ô∏è  Could not find position embeddings to extend")
        
    def _replace_attention(self):
        """Replace attention layers with GSA"""
        print(f"Replacing {len(self.model.transformer.h)} attention layers with GSA...")
        
        # Disable model caching to avoid compatibility issues
        self.model.config.use_cache = False
        
        for i, layer in enumerate(self.model.transformer.h):
            original_attn = layer.attn
            gsa_layer = GSALayer(self.gsa_config)
            
            # Try to copy weights if possible
            try:
                if hasattr(original_attn, 'c_attn'):
                    # GPT-2 style
                    gsa_layer.attention.qkv_proj.weight.data.copy_(original_attn.c_attn.weight.data)
                elif hasattr(original_attn, 'q_proj'):
                    # GPT-Neo style - TinyStories uses this
                    q_w = original_attn.q_proj.weight.data
                    k_w = original_attn.k_proj.weight.data  
                    v_w = original_attn.v_proj.weight.data
                    qkv_w = torch.cat([q_w, k_w, v_w], dim=0)
                    gsa_layer.attention.qkv_proj.weight.data.copy_(qkv_w)
                    
                if hasattr(original_attn, 'c_proj'):
                    gsa_layer.attention.out_proj.weight.data.copy_(original_attn.c_proj.weight.data)
                elif hasattr(original_attn, 'out_proj'):
                    gsa_layer.attention.out_proj.weight.data.copy_(original_attn.out_proj.weight.data)
                    
                print(f"  Layer {i}: Copied weights successfully")
            except Exception as e:
                print(f"  Layer {i}: Could not copy weights ({e}), using random init")
            
            # Replace the layer - ensure we maintain all necessary attributes
            layer.attn = gsa_layer
        
        print("GSA replacement complete!")
    
    def forward(self, input_ids, attention_mask=None, **kwargs):
        return self.model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)
    
    def generate(self, **kwargs):
        return self.model.generate(**kwargs)

def create_passkey_test(tokenizer, context_length=1024, passkey_pos="middle"):
    """Create a passkey retrieval test with actual target length"""
    
    # Generate passkey
    passkey = f"{np.random.randint(1000, 9999)}"
    passkey_text = f"The secret code is {passkey}."
    
    # Create base filler sentences with more variety
    base_sentences = [
        "The quick brown fox jumps over the lazy dog and runs through the meadow.",
        "In the ancient forest, tall trees sway gently in the warm summer breeze.",
        "Children played happily in the park while their parents watched from nearby benches.",
        "The mysterious castle stood on the hilltop, surrounded by thick fog and legends.",
        "Merchants traveled along the dusty road, carrying goods from distant lands.",
        "The old lighthouse keeper maintained his watch through stormy nights at sea.",
        "Students gathered in the library, studying for their important examinations ahead.",
        "The baker woke early each morning to prepare fresh bread for the village.",
        "Pirates sailed across the ocean searching for treasure on remote islands.",
        "The wise wizard studied ancient spells in his tower filled with magical books."
    ]
    
    # Calculate how much filler we need
    question = "\n\nWhat is the secret code mentioned in the text? The secret code is:"
    
    # Estimate tokens needed (rough approximation: 1 token per 4 characters)
    passkey_tokens = len(tokenizer.encode(passkey_text))
    question_tokens = len(tokenizer.encode(question))
    target_filler_tokens = context_length - passkey_tokens - question_tokens - 20  # 20 token buffer
    
    # Generate enough filler to reach target length
    filler_text = ""
    current_tokens = 0
    sentence_idx = 0
    
    while current_tokens < target_filler_tokens:
        sentence = base_sentences[sentence_idx % len(base_sentences)]
        filler_text += sentence + " "
        current_tokens = len(tokenizer.encode(filler_text))
        sentence_idx += 1
    
    # Position the passkey
    if passkey_pos == "beginning":
        text = passkey_text + " " + filler_text
    elif passkey_pos == "middle":
        # Find middle point in terms of tokens, not characters
        words = filler_text.split()
        mid_point = len(words) // 2
        first_half = " ".join(words[:mid_point])
        second_half = " ".join(words[mid_point:])
        text = first_half + " " + passkey_text + " " + second_half
    else:  # end
        text = filler_text + " " + passkey_text
    
    # Add question
    prompt = text + question
    
    # Verify actual length
    actual_tokens = len(tokenizer.encode(prompt))
    
    return prompt, passkey, actual_tokens

def test_context_extension():
    """Main test function"""
    print("üß™ GSA Context Extension Test")
    print("=" * 50)
    
    # Load model and tokenizer
    model_name = "roneneldan/TinyStories-Instruct-33M"
    print(f"Loading {model_name}...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)
    base_model.config.use_cache = False  # Disable caching for GSA compatibility
    original_max_length = getattr(base_model.config, 'max_position_embeddings', 2048)
    
    print(f"Original max position embeddings: {original_max_length}")
    
    # Move to GPU if available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    base_model = base_model.to(device)
    print(f"Using device: {device}")
    
    # Create GSA config
    gsa_config = GSAConfig(
        dim=base_model.config.hidden_size,
        n_heads=base_model.config.num_attention_heads,
        n_splats_per_head=16,  # More splats for longer context
        movement_scale=0.1,
        temperature_init=1.0
    )
    
    print(f"GSA Config: dim={gsa_config.dim}, heads={gsa_config.n_heads}, splats={gsa_config.n_splats_per_head}")
    
    # Create BOTH models for comparison
    print("\nüîÑ Creating models for comparison...")
    
    # 1. Original model with extended positions (but standard attention)
    print("Extending standard attention model positions...")
    original_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)
    original_model.config.use_cache = False
    
    # Extend positions for fair comparison
    if hasattr(original_model.transformer, 'wpe'):
        old_pos_emb = original_model.transformer.wpe.weight.data
        embed_dim = old_pos_emb.shape[1]
        new_max_pos = 16384
        
        new_pos_emb = nn.Embedding(new_max_pos, embed_dim)
        new_pos_emb.weight.data[:original_max_length] = old_pos_emb
        
        # Better extrapolation for new positions
        for i in range(original_max_length, new_max_pos):
            old_idx = i % original_max_length
            new_pos_emb.weight.data[i] = old_pos_emb[old_idx] * 0.95
        
        original_model.transformer.wpe = new_pos_emb
        original_model.config.max_position_embeddings = new_max_pos
        original_model.config.n_positions = new_max_pos  # Also update n_positions if it exists
        
        print(f"  ‚úÖ Extended standard model positions: {original_max_length} ‚Üí {new_max_pos}")
    else:
        print("  ‚ö†Ô∏è  Could not extend positions for standard model")
    
    original_model = original_model.to(device)
    
    # 2. GSA model with extended positions
    print("Creating GSA model with extended positions...")
    gsa_model = GSAModelWrapper(base_model, gsa_config, extend_positions=True).to(device)
    
    # COMPARISON TEST - Test both models on same context length
    print(f"\nüî¨ DIRECT COMPARISON TEST")
    print("=" * 60)
    
    comparison_length = 3072  # Test at 3K tokens (beyond original 2048 limit)
    print(f"Testing both models at {comparison_length} tokens...")
    
    # Create test case
    prompt, expected_passkey, actual_tokens = create_passkey_test(tokenizer, comparison_length, "middle")
    inputs = tokenizer(prompt, return_tensors="pt", max_length=comparison_length + 100, truncation=True).to(device)
    
    print(f"Test case: {actual_tokens} tokens, passkey='{expected_passkey}'")
    print(f"Context length: {comparison_length} (beyond original {original_max_length} limit)")
    
    # Test original model
    print(f"\nüìä STANDARD ATTENTION MODEL:")
    print("   (Testing if standard attention can handle extended context)")
    try:
        with torch.no_grad():
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            start_time = time.time()
            
            orig_outputs = original_model.generate(
                input_ids=inputs['input_ids'],
                attention_mask=inputs.get('attention_mask'),
                max_new_tokens=10,
                temperature=0.1,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                use_cache=False
            )
            
            orig_time = time.time() - start_time
            orig_response = tokenizer.decode(orig_outputs[0], skip_special_tokens=True)
            orig_prompt_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)
            
            if len(orig_response) > len(orig_prompt_text):
                orig_generated = orig_response[len(orig_prompt_text):].strip()
            else:
                orig_generated = orig_response.strip()
            
            orig_success = expected_passkey in orig_response
            
            print(f"  ‚úÖ SUCCEEDED at {comparison_length} tokens!")
            print(f"  Time: {orig_time:.2f}s")
            print(f"  Generated: '{orig_generated}'")
            print(f"  Passkey found: {'‚úÖ' if orig_success else '‚ùå'}")
            
    except Exception as e:
        print(f"  ‚ùå FAILED: {str(e)[:100]}...")
        if "size of tensor" in str(e) or "index" in str(e).lower():
            print(f"  üí° This is the expected failure - standard attention can't handle {comparison_length} tokens")
        orig_success = False
        orig_time = float('inf')
        orig_generated = "FAILED"
    
    # Test GSA model
    print(f"\nüéØ GSA ATTENTION MODEL:")
    print("   (Testing if GSA can handle extended context where standard attention fails)")
    try:
        with torch.no_grad():
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            start_time = time.time()
            
            gsa_outputs = gsa_model.generate(
                input_ids=inputs['input_ids'],
                attention_mask=inputs.get('attention_mask'),
                max_new_tokens=10,
                temperature=0.1,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                use_cache=False
            )
            
            gsa_time = time.time() - start_time
            gsa_response = tokenizer.decode(gsa_outputs[0], skip_special_tokens=True)
            gsa_prompt_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)
            
            if len(gsa_response) > len(gsa_prompt_text):
                gsa_generated = gsa_response[len(gsa_prompt_text):].strip()
            else:
                gsa_generated = gsa_response.strip()
            
            gsa_success = expected_passkey in gsa_response
            
            print(f"  ‚úÖ SUCCEEDED at {comparison_length} tokens!")
            print(f"  Time: {gsa_time:.2f}s")
            print(f"  Generated: '{gsa_generated}'")
            print(f"  Passkey found: {'‚úÖ' if gsa_success else '‚ùå'}")
            
    except Exception as e:
        print(f"  ‚ùå FAILED: {str(e)[:100]}...")
        gsa_success = False
        gsa_time = float('inf')
        gsa_generated = "FAILED"
    
    # Show comparison
    print(f"\nüèÜ CONTEXT EXTENSION COMPARISON:")
    print(f"  üìè Test Length: {comparison_length} tokens (vs original {original_max_length} limit)")
    print(f"  üîß Standard Attention: {'‚úÖ SUCCESS' if orig_success else '‚ùå FAILED'}")
    print(f"  üéØ GSA Attention:      {'‚úÖ SUCCESS' if gsa_success else '‚ùå FAILED'}")
    
    if not orig_success and gsa_success:
        print(f"  üéâ CONTEXT EXTENSION ACHIEVED!")
        print(f"     GSA succeeded where standard attention failed!")
        extension_factor = comparison_length / original_max_length
        print(f"     Extension factor: {extension_factor:.1f}x")
    elif orig_success and gsa_success:
        speedup = orig_time / gsa_time if gsa_time > 0 else 1.0
        print(f"  ‚ö° Both succeeded, GSA was {speedup:.2f}x {'faster' if speedup > 1 else 'slower'}")
    elif not orig_success and not gsa_success:
        print(f"  ‚ö†Ô∏è  Both models failed at this length")
    else:
        print(f"  ü§î Unexpected result pattern")
    
    # Verify GSA is actually being used
    print(f"\nüîç VERIFICATION - GSA Actually Being Used:")
    first_layer = gsa_model.model.transformer.h[0].attn
    if hasattr(first_layer, 'attention') and hasattr(first_layer.attention, 'splat_centers'):
        splat_centers = first_layer.attention.splat_centers
        print(f"  ‚úÖ GSA splat centers shape: {splat_centers.shape}")
        print(f"  ‚úÖ GSA splats detected: {splat_centers.shape[0]} heads √ó {splat_centers.shape[1]} splats")
        
        # Show some splat statistics
        with torch.no_grad():
            center_norms = torch.norm(splat_centers, dim=-1)
            print(f"  üìä Splat center norms: min={center_norms.min():.3f}, max={center_norms.max():.3f}, mean={center_norms.mean():.3f}")
    else:
        print(f"  ‚ùå WARNING: GSA components not detected!")
    
    print(f"\n" + "="*60)
    
    # Now continue with progressive testing
    test_lengths = []
    current_length = 4096  # Start higher since we proved GSA works at 3072
    max_attempts = 15
    
    print(f"Starting progressive testing at {current_length} tokens")
    print(f"(Already verified GSA works better than standard attention at 3072)")
    
    for attempt in range(max_attempts):
        test_lengths.append(current_length)
        current_length += 1024
    results = {}
    failed_length = None
    
    for ctx_len in test_lengths:
        print(f"\nüìè Testing context length: {ctx_len}")
        print("-" * 30)
        
        # Check available GPU memory before testing
        if torch.cuda.is_available():
            memory_before = torch.cuda.memory_allocated() / 1024**2
            print(f"GPU memory before test: {memory_before:.1f}MB")
        
        success_count = 0
        total_tests = 3  # Reduced for faster testing
        test_failed = False
        
        for i in range(total_tests):
            # Create test case
            prompt, expected_passkey, actual_tokens = create_passkey_test(tokenizer, ctx_len, "middle")
            
            print(f"Test {i+1}: Target={ctx_len}, Actual={actual_tokens} tokens, Passkey='{expected_passkey}'")
            
            # Show a sample of the generated text structure
            if i == 0:  # Only for first test to avoid spam
                print(f"    üìù FULL PROMPT PREVIEW:")
                print(f"    " + "="*60)
                
                # Show beginning of prompt
                lines = prompt.split('\n')
                print(f"    Beginning: {prompt[:300]}...")
                print(f"    ...")
                
                # Show where passkey is located
                passkey_location = prompt.find(f"The secret code is {expected_passkey}")
                if passkey_location > 0:
                    context_start = max(0, passkey_location - 100)
                    context_end = min(len(prompt), passkey_location + 150)
                    passkey_context = prompt[context_start:context_end]
                    print(f"    Passkey location (chars {passkey_location}):")
                    print(f"    ...{passkey_context}...")
                
                # Show the question at the end
                question_start = prompt.rfind("What is the secret code")
                if question_start > 0:
                    question_part = prompt[question_start:]
                    print(f"    Question: {question_part}")
                
                print(f"    " + "="*60)
                print(f"    üìä PROMPT STATS:")
                print(f"    - Total characters: {len(prompt):,}")
                print(f"    - Total words: {len(prompt.split()):,}")
                print(f"    - Passkey position: {passkey_location:,} chars ({passkey_location/len(prompt)*100:.1f}% through text)")
                print(f"    " + "="*60)
            
            # Tokenize - don't truncate, we want to test the actual length
            try:
                inputs = tokenizer(prompt, return_tensors="pt", max_length=ctx_len + 100, truncation=True).to(device)
                input_length = inputs['input_ids'].shape[1]
                
                # Check if we're actually testing the target length
                if input_length < ctx_len * 0.8:  # If significantly shorter than target
                    print(f"    ‚ö†Ô∏è  WARNING: Only {input_length} tokens after tokenization, target was {ctx_len}")
                
            except Exception as e:
                print(f"  ‚ùå TOKENIZATION ERROR: {e}")
                test_failed = True
                break
            
            # Test with GSA model
            try:
                # Show the full generation process
                print(f"    ü§ñ GENERATION PROCESS:")
                print(f"    Input tokens: {input_length}")
                print(f"    Generating max 10 new tokens...")
                
                # Time the generation
                gen_start = time.time()
                
                with torch.no_grad():
                    # Clear cache before each test
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    outputs = gsa_model.generate(
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs.get('attention_mask'),
                        max_new_tokens=10,
                        temperature=0.1,
                        do_sample=True,
                        pad_token_id=tokenizer.pad_token_id,
                        use_cache=False
                    )
                
                gen_time = time.time() - gen_start
                print(f"    Generation time: {gen_time:.2f}s")
                
                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # Extract just the generated part (after the prompt)
                prompt_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)
                if len(response) > len(prompt_text):
                    generated_part = response[len(prompt_text):].strip()
                else:
                    generated_part = response.strip()
                
                print(f"    Generated: '{generated_part}'")
                
                # Check if passkey is in response
                if expected_passkey in response:
                    success_count += 1
                    print(f"  ‚úÖ SUCCESS - Found passkey '{expected_passkey}' in response")
                else:
                    print(f"  ‚ö†Ô∏è  PARTIAL - Expected: '{expected_passkey}', got: '{generated_part}'")
                    # Check if it's close (sometimes model generates partial numbers)
                    if any(digit in generated_part for digit in expected_passkey):
                        print(f"      (Contains some digits from expected passkey)")
                    
                    # Show a bit of context around where passkey should be
                    passkey_phrase = f"The secret code is {expected_passkey}"
                    if passkey_phrase in prompt_text:
                        idx = prompt_text.find(passkey_phrase)
                        context_start = max(0, idx - 50)
                        context_end = min(len(prompt_text), idx + len(passkey_phrase) + 50)
                        context = prompt_text[context_start:context_end].replace('\n', ' ')
                        print(f"      Context: '...{context}...'")
                    
                    
            except RuntimeError as e:
                error_msg = str(e).lower()
                if "out of memory" in error_msg:
                    print(f"  ‚ùå OUT OF MEMORY at context length {ctx_len}")
                    test_failed = True
                    failed_length = ctx_len
                    break
                elif "cuda error" in error_msg or "assert" in error_msg:
                    print(f"  ‚ùå CUDA ERROR at context length {ctx_len}: {e}")
                    test_failed = True
                    failed_length = ctx_len
                    break
                elif "index out of range" in error_msg or "position" in error_msg:
                    print(f"  ‚ùå POSITION ERROR at context length {ctx_len}: {e}")
                    test_failed = True
                    failed_length = ctx_len
                    break
                else:
                    print(f"  ‚ùå RUNTIME ERROR: {e}")
                    test_failed = True
                    break
            except Exception as e:
                print(f"  ‚ùå UNEXPECTED ERROR: {e}")
                test_failed = True
                break
        
        if test_failed:
            failed_length = ctx_len
            print(f"üí• FAILED at context length {ctx_len}")
            break
        
        accuracy = success_count / total_tests
        results[ctx_len] = {
            'accuracy': accuracy,
            'success_count': success_count,
            'total_tests': total_tests,
            'status': 'completed'
        }
        
        print(f"üìä Context {ctx_len}: {success_count}/{total_tests} = {accuracy:.1%} accuracy")
        
        # Check GPU memory after test
        if torch.cuda.is_available():
            memory_after = torch.cuda.memory_allocated() / 1024**2
            print(f"GPU memory after test: {memory_after:.1f}MB (Œî{memory_after-memory_before:+.1f}MB)")
        
        # Stop if accuracy drops too low (indicates model is struggling)
        if accuracy < 0.2:
            print(f"‚ö†Ô∏è  Accuracy dropped to {accuracy:.1%}, model may be struggling with this length")
            # But continue testing to find the hard limit
    
    # Summary
    print(f"\n{'=' * 60}")
    print("üìà PROGRESSIVE CONTEXT LENGTH TEST RESULTS")
    print(f"{'=' * 60}")
    
    max_successful = 0
    for ctx_len, result in results.items():
        if result['status'] == 'completed':
            max_successful = ctx_len
            status = "‚úÖ" if result['accuracy'] > 0.6 else "‚ö†Ô∏è" if result['accuracy'] > 0.3 else "‚ùå"
            print(f"{status} Context {ctx_len}: {result['accuracy']:.1%} accuracy ({result['success_count']}/{result['total_tests']})")
    
    if failed_length:
        print(f"üí• FAILURE at context length: {failed_length}")
        print(f"üèÜ MAXIMUM SUCCESSFUL length: {max_successful}")
        extension_ratio = max_successful / original_max_length
        print(f"üìè Context extension: {original_max_length} ‚Üí {max_successful} ({extension_ratio:.1f}x)")
    else:
        print(f"üéâ ALL TESTS PASSED up to {max(results.keys() if results else [0])}")
        if results:
            extension_ratio = max(results.keys()) / original_max_length
            print(f"üìè Context extension: {original_max_length} ‚Üí {max(results.keys())} ({extension_ratio:.1f}x)")
    
    # GPU memory summary
    if torch.cuda.is_available():
        final_memory = torch.cuda.memory_allocated() / 1024**2
        max_memory = torch.cuda.max_memory_allocated() / 1024**2
        print(f"üîã Final GPU memory: {final_memory:.1f}MB (Peak: {max_memory:.1f}MB)")
    
    return results, failed_length

if __name__ == "__main__":
    try:
        results, failed_length = test_context_extension()
        
        if failed_length:
            print(f"\nüîç GSA Context Extension Limit Found: {failed_length}")
        else:
            print(f"\n‚úÖ Test completed - no failures detected!")
            
    except Exception as e:
        print(f"\n‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()
