"""
SplatFlow Real Data Training - Testing if SplatFlow can work on real datasets

This script combines:
- The proven O(n*k) SplatFlow implementation 
- Extended training with real datasets
- Proper evaluation and monitoring

Goal: See if SplatFlow can generate coherent text when trained on real data
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
import time
import json
import os
import gc
import random
from typing import Tuple, Optional, Dict, List
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer
from datasets import load_dataset

# Enable memory optimizations
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
torch.backends.cudnn.benchmark = True
torch.backends.cuda.matmul.allow_tf32 = True


def cleanup_memory():
    """Aggressive memory cleanup"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


def get_gpu_memory_info():
    """Get current GPU memory usage"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        free = total - reserved
        return {
            'allocated': allocated,
            'reserved': reserved,
            'total': total,
            'free': free,
            'percent_used': (allocated / total) * 100
        }
    return None


# ==================== SPLATFLOW IMPLEMENTATION ====================

class TrueSplatAttentionLayer(nn.Module):
    """
    O(n*k) Splat Attention Layer - The Core Breakthrough
    
    Tokens communicate exclusively through splats:
    1. Tokenâ†’Splat: Aggregate token information at splat locations O(n*k)
    2. Splat Processing: Optional transformation of splat states O(k) 
    3. Splatâ†’Token: Distribute splat information back to tokens O(n*k)
    
    Total: O(n*k) - Linear in sequence length!
    """
    
    def __init__(self, model_dim: int, num_splats: int = 16, 
                 splat_dim: Optional[int] = None, enable_splat_mlp: bool = False,
                 dropout: float = 0.1, temperature: float = 1.0):
        super().__init__()
        
        self.model_dim = model_dim
        self.num_splats = num_splats
        self.splat_dim = splat_dim or model_dim
        self.enable_splat_mlp = enable_splat_mlp
        self.temperature = temperature
        
        # Splat parameters - learnable positions and scales in embedding space
        self.splat_centers = nn.Parameter(torch.randn(num_splats, model_dim) * 0.02)
        self.splat_log_scales = nn.Parameter(torch.zeros(num_splats))  # log(scale) for stability
        
        # Projections for token values (like V in standard attention)
        self.token_value_proj = nn.Linear(model_dim, self.splat_dim, bias=False)
        
        # Optional: Splat processing MLP
        if enable_splat_mlp:
            self.splat_mlp = nn.Sequential(
                nn.Linear(self.splat_dim, self.splat_dim * 2),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(self.splat_dim * 2, self.splat_dim)
            )
        else:
            self.splat_mlp = nn.Identity()
        
        # Output projection (like O in standard attention)
        self.output_proj = nn.Linear(self.splat_dim, model_dim, bias=False)
        
        # Dropout for regularization
        self.dropout = nn.Dropout(dropout)
        
        # Initialize weights
        self._init_weights()
    
    def _init_weights(self):
        """Initialize weights with careful scaling"""
        # Initialize splat centers with small random values
        nn.init.normal_(self.splat_centers, mean=0.0, std=0.02)
        
        # Initialize log scales to give reasonable initial spread
        nn.init.constant_(self.splat_log_scales, math.log(0.5))
        
        # Initialize projections
        nn.init.xavier_uniform_(self.token_value_proj.weight)
        nn.init.xavier_uniform_(self.output_proj.weight)
    
    def compute_affinity_matrix(self, token_embeddings: torch.Tensor) -> torch.Tensor:
        """
        Compute affinities between tokens and splats - O(n*k) operation
        
        Args:
            token_embeddings: [batch, seq_len, model_dim]
            
        Returns:
            affinities: [batch, seq_len, num_splats] - how much each token communicates with each splat
        """
        batch_size, seq_len, model_dim = token_embeddings.shape
        
        # Compute squared distances: ||token - splat_center||Â²
        # token_embeddings: [batch, seq_len, model_dim]
        # splat_centers: [num_splats, model_dim]
        
        # Expand for broadcasting
        tokens_expanded = token_embeddings.unsqueeze(2)  # [batch, seq_len, 1, model_dim]
        centers_expanded = self.splat_centers.unsqueeze(0).unsqueeze(0)  # [1, 1, num_splats, model_dim]
        
        # Compute squared distances
        diff = tokens_expanded - centers_expanded  # [batch, seq_len, num_splats, model_dim]
        distances_sq = torch.sum(diff ** 2, dim=-1)  # [batch, seq_len, num_splats]
        
        # Apply learned scales (convert from log space)
        scales = torch.exp(self.splat_log_scales).clamp(min=0.1, max=2.0)  # [num_splats]
        scales_sq = scales ** 2  # [num_splats]
        
        # Compute Gaussian affinities
        affinities = torch.exp(-0.5 * distances_sq / scales_sq.unsqueeze(0).unsqueeze(0))
        
        # Apply temperature scaling
        affinities = affinities ** (1.0 / self.temperature)
        
        # Normalize affinities (each token's affinities sum to 1)
        affinities = affinities / (affinities.sum(dim=-1, keepdim=True) + 1e-8)
        
        return affinities  # [batch, seq_len, num_splats]
    
    def forward(self, token_embeddings: torch.Tensor, 
                attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        True O(n*k) splat attention forward pass
        
        Args:
            token_embeddings: [batch, seq_len, model_dim]
            attention_mask: Optional[batch, seq_len] - not used in current implementation
            
        Returns:
            output: [batch, seq_len, model_dim]
        """
        batch_size, seq_len, model_dim = token_embeddings.shape
        
        # Phase 1: Compute token-splat affinities O(n*k)
        affinities = self.compute_affinity_matrix(token_embeddings)  # [batch, seq_len, num_splats]
        
        # Project token embeddings to values
        token_values = self.token_value_proj(token_embeddings)  # [batch, seq_len, splat_dim]
        
        # Phase 2: Aggregate information at splats O(n*k*d)
        # This is matrix multiplication: affinities.T @ token_values
        splat_states = torch.einsum('bsk,bsd->bkd', affinities, token_values)  # [batch, num_splats, splat_dim]
        
        # Phase 3: Optional splat processing O(k*d)
        splat_states = self.splat_mlp(splat_states)  # [batch, num_splats, splat_dim]
        
        # Phase 4: Distribute information back to tokens O(n*k*d)  
        # This is matrix multiplication: affinities @ splat_states
        token_outputs = torch.einsum('bsk,bkd->bsd', affinities, splat_states)  # [batch, seq_len, splat_dim]
        
        # Apply dropout
        token_outputs = self.dropout(token_outputs)
        
        # Final output projection
        output = self.output_proj(token_outputs)  # [batch, seq_len, model_dim]
        
        return output


class SparseSplatAttentionLayer(TrueSplatAttentionLayer):
    """
    Sparse variant with top-k splat selection for even better efficiency
    Complexity: O(n*p) where p << k (typically p = 2-4)
    """
    
    def __init__(self, *args, top_k_splats: int = 4, **kwargs):
        super().__init__(*args, **kwargs)
        self.top_k_splats = min(top_k_splats, self.num_splats)
    
    def compute_affinity_matrix(self, token_embeddings: torch.Tensor) -> torch.Tensor:
        """Compute sparse affinities - only top-k splats per token"""
        # First compute all affinities
        full_affinities = super().compute_affinity_matrix(token_embeddings)  # [batch, seq_len, num_splats]
        
        # Select top-k splats for each token
        top_k_values, top_k_indices = torch.topk(full_affinities, self.top_k_splats, dim=-1)
        
        # Create sparse affinity matrix
        sparse_affinities = torch.zeros_like(full_affinities)
        
        # Scatter top-k values back to sparse matrix
        batch_indices = torch.arange(full_affinities.size(0)).unsqueeze(1).unsqueeze(2)
        seq_indices = torch.arange(full_affinities.size(1)).unsqueeze(0).unsqueeze(2)
        
        sparse_affinities[batch_indices, seq_indices, top_k_indices] = top_k_values
        
        # Renormalize
        sparse_affinities = sparse_affinities / (sparse_affinities.sum(dim=-1, keepdim=True) + 1e-8)
        
        return sparse_affinities


class SplatFlowTransformerLayer(nn.Module):
    """Complete transformer layer using O(n*k) splat attention"""
    
    def __init__(self, model_dim: int, num_splats: int = 16, 
                 ff_dim: Optional[int] = None, dropout: float = 0.1,
                 use_sparse_splats: bool = False, top_k_splats: int = 4):
        super().__init__()
        
        if ff_dim is None:
            ff_dim = model_dim * 4
        
        # Choose attention type
        if use_sparse_splats:
            self.attention = SparseSplatAttentionLayer(
                model_dim, num_splats, dropout=dropout, top_k_splats=top_k_splats
            )
        else:
            self.attention = TrueSplatAttentionLayer(
                model_dim, num_splats, dropout=dropout
            )
        
        # Layer norms
        self.attn_norm = nn.LayerNorm(model_dim)
        self.ff_norm = nn.LayerNorm(model_dim)
        
        # Feed-forward network
        self.feed_forward = nn.Sequential(
            nn.Linear(model_dim, ff_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(ff_dim, model_dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Standard transformer layer forward pass with splat attention"""
        
        # Self-attention with residual connection
        attn_output = self.attention(x, attention_mask)
        x = self.attn_norm(x + attn_output)
        
        # Feed-forward with residual connection  
        ff_output = self.feed_forward(x)
        x = self.ff_norm(x + ff_output)
        
        return x


class SplatFlowGPT(nn.Module):
    """
    Complete GPT model using O(n*k) splat attention
    
    This model achieves O(n*k*d) complexity instead of O(nÂ²*d),
    enabling training on much longer sequences with the same memory.
    """
    
    def __init__(self, vocab_size: int, model_dim: int = 512, num_layers: int = 6,
                 num_splats: int = 16, max_seq_len: int = 1024, dropout: float = 0.1,
                 use_sparse_splats: bool = False, top_k_splats: int = 4):
        super().__init__()
        
        self.model_dim = model_dim
        self.num_layers = num_layers
        self.num_splats = num_splats
        self.max_seq_len = max_seq_len
        
        # Embeddings
        self.token_embedding = nn.Embedding(vocab_size, model_dim)
        self.position_embedding = nn.Embedding(max_seq_len, model_dim)
        self.embedding_dropout = nn.Dropout(dropout)
        
        # Transformer layers with splat attention
        self.layers = nn.ModuleList([
            SplatFlowTransformerLayer(
                model_dim, num_splats, dropout=dropout,
                use_sparse_splats=use_sparse_splats,
                top_k_splats=top_k_splats
            ) for _ in range(num_layers)
        ])
        
        # Output
        self.final_norm = nn.LayerNorm(model_dim)
        self.output_projection = nn.Linear(model_dim, vocab_size, bias=False)
        
        # Initialize weights
        self.apply(self._init_weights)
        
        # Report model statistics
        self._report_model_stats()
    
    def _init_weights(self, module):
        """Initialize model weights"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def _report_model_stats(self):
        """Report model complexity statistics"""
        total_params = sum(p.numel() for p in self.parameters())
        
        print(f"SplatFlow Model Statistics:")
        print(f"  Total parameters: {total_params:,}")
        print(f"  Layers: {self.num_layers}")
        print(f"  Splats per layer: {self.num_splats}")
        print(f"  Model dimension: {self.model_dim}")
        print(f"  Theoretical complexity: O(n*{self.num_splats}*{self.model_dim}) per layer")
    
    def forward(self, input_ids: torch.Tensor, 
                attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Forward pass through the splat-flow model"""
        batch_size, seq_len = input_ids.shape
        device = input_ids.device
        
        # Embeddings
        token_emb = self.token_embedding(input_ids)
        pos_ids = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)
        pos_emb = self.position_embedding(pos_ids)
        
        x = self.embedding_dropout(token_emb + pos_emb)
        
        # Process through splat-flow layers
        for layer in self.layers:
            x = layer(x, attention_mask)
        
        # Output
        x = self.final_norm(x)
        logits = self.output_projection(x)
        
        return logits


# ==================== DATASET LOADING ====================

class RealDataset(Dataset):
    """Dataset that loads real text data from multiple sources"""
    
    def __init__(self, tokenizer, seq_length: int = 1024, total_sequences: int = 2000):
        self.tokenizer = tokenizer
        self.seq_length = seq_length
        self.examples = []
        
        print(f"ğŸ“š Creating dataset with {total_sequences} sequences of {seq_length} tokens")
        
        # Collect texts from multiple sources
        all_texts = []
        
        # 1. TinyStories - Known to work well
        all_texts.extend(self.load_tinystories(target_texts=total_sequences//3))
        
        # 2. WikiText-103 - Good quality articles
        all_texts.extend(self.load_wikitext(target_texts=total_sequences//3))
        
        # 3. OpenWebText - If available
        all_texts.extend(self.load_openwebtext(target_texts=total_sequences//4))
        
        # 4. Fill remainder with quality synthetic
        current_count = len(all_texts)
        remaining = max(total_sequences//2 - current_count, 200)  # Ensure minimum
        all_texts.extend(self.create_quality_synthetic(remaining))
        
        print(f"ğŸ“Š Total source texts collected: {len(all_texts)}")
        
        # Create sequences
        self.create_sequences_from_texts(all_texts, total_sequences)
        
        print(f"âœ… Final dataset: {len(self.examples)} sequences")
    
    def load_tinystories(self, target_texts: int) -> List[str]:
        """Load TinyStories"""
        texts = []
        try:
            print(f"  ğŸ“– Loading TinyStories (target: {target_texts})...")
            dataset = load_dataset("roneneldan/TinyStories", split="train")
            
            count = 0
            for item in dataset:
                if count >= target_texts:
                    break
                    
                text = item['text'].strip()
                if len(text) > 200:
                    texts.append(text + "\n\n")
                    count += 1
            
            print(f"    âœ… Added {len(texts)} TinyStories")
            
        except Exception as e:
            print(f"    âŒ Failed to load TinyStories: {e}")
        
        return texts
    
    def load_wikitext(self, target_texts: int) -> List[str]:
        """Load WikiText-103"""
        texts = []
        try:
            print(f"  ğŸ“– Loading WikiText-103 (target: {target_texts})...")
            dataset = load_dataset("wikitext", "wikitext-103-raw-v1", split="train")
            
            count = 0
            for item in dataset:
                if count >= target_texts:
                    break
                    
                text = item['text'].strip()
                if len(text) > 500 and not text.startswith('='):  # Skip headers
                    texts.append(text + "\n\n")
                    count += 1
            
            print(f"    âœ… Added {len(texts)} WikiText articles")
            
        except Exception as e:
            print(f"    âŒ Failed to load WikiText: {e}")
        
        return texts
    
    def load_openwebtext(self, target_texts: int) -> List[str]:
        """Load OpenWebText if available"""
        texts = []
        try:
            print(f"  ğŸ“– Loading OpenWebText (target: {target_texts})...")
            dataset = load_dataset("openwebtext", split="train")
            
            count = 0
            for item in dataset:
                if count >= target_texts:
                    break
                    
                text = item['text'].strip()
                if 300 < len(text) < 5000:  # Medium length articles
                    texts.append(text + "\n\n")
                    count += 1
            
            print(f"    âœ… Added {len(texts)} OpenWebText articles")
            
        except Exception as e:
            print(f"    âŒ Failed to load OpenWebText: {e}")
        
        return texts
    
    def create_quality_synthetic(self, target_texts: int) -> List[str]:
        """Create synthetic texts"""
        print(f"  ğŸ¤– Creating {target_texts} synthetic texts...")
        
        templates = [
            """The field of {topic} has seen remarkable progress recently. Scientists have discovered {finding}, which could revolutionize {application}.

This breakthrough builds on previous work in {related_field}. The key insight is that {insight}, enabling researchers to {capability}.

Practical applications include {use_case1} and {use_case2}. For instance, {example} demonstrates the potential for real-world impact.

Looking forward, experts predict {prediction}. The next steps involve {next_steps} and addressing challenges in {challenge_area}.""",

            """In a small village nestled between rolling hills, there lived a {character} who had an unusual gift. Every {time_period}, {character} could {ability}.

One day, a {visitor} arrived seeking help with {problem}. "{character}," said the {visitor}, "{request}."

At first, {character} was hesitant. {reason_for_hesitation}. But seeing the {visitor}'s distress, {character} decided to help.

The journey was not easy. They encountered {obstacle1} and had to overcome {obstacle2}. Through {method}, they learned {lesson}.

In the end, {outcome}. The {visitor} was grateful, and {character} realized {moral}."""
        ]
        
        topics = ["artificial intelligence", "renewable energy", "space exploration", "medicine", "education"]
        
        texts = []
        for i in range(target_texts):
            template = random.choice(templates)
            topic = random.choice(topics)
            
            filled_text = template.format(
                topic=topic,
                finding="unexpected patterns in large-scale data",
                application="how we solve complex problems",
                related_field="computational science",
                insight="complex systems follow simple principles",
                capability="predict outcomes with greater accuracy",
                use_case1="climate modeling",
                use_case2="disease prevention",
                example="recent cancer research",
                prediction="these technologies will become mainstream",
                next_steps="developing better algorithms",
                challenge_area="ethical deployment",
                
                # Story elements
                character="wise healer",
                time_period="full moon",
                ability="see the future in dreams",
                visitor="desperate merchant",
                problem="a terrible curse",
                request="please help me save my family",
                reason_for_hesitation="the visions were often unclear",
                obstacle1="treacherous mountain paths",
                obstacle2="ancient guardians",
                method="courage and wisdom",
                lesson="that true power comes from helping others",
                outcome="the curse was broken",
                moral="that every gift should be used for good"
            )
            
            texts.append(filled_text + "\n\n")
        
        print(f"    âœ… Created {len(texts)} synthetic texts")
        return texts
    
    def create_sequences_from_texts(self, texts: List[str], target_sequences: int):
        """Create sequences from texts"""
        print(f"  ğŸ”§ Processing texts into {self.seq_length}-token sequences...")
        
        # Tokenize all texts
        all_tokens = []
        for i, text in enumerate(texts):
            if i % 200 == 0:
                print(f"    Processing text {i+1}/{len(texts)}...")
                
            try:
                tokens = self.tokenizer.encode(text, add_special_tokens=True)
                all_tokens.extend(tokens)
                all_tokens.append(self.tokenizer.eos_token_id)
            except:
                continue
        
        print(f"    ğŸ“Š Total tokens: {len(all_tokens):,}")
        
        # Create sequences
        sequences_created = 0
        for start_idx in range(0, len(all_tokens) - self.seq_length, self.seq_length):
            if sequences_created >= target_sequences:
                break
                
            sequence = all_tokens[start_idx:start_idx + self.seq_length]
            if len(sequence) == self.seq_length:
                self.examples.append(torch.tensor(sequence, dtype=torch.long))
                sequences_created += 1
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        return self.examples[idx]


# ==================== TRAINING FUNCTIONS ====================

def test_generation(model, tokenizer, prompts: List[str], device, max_tokens: int = 40):
    """Test generation quality"""
    model.eval()
    
    print("ğŸ¯ Generation Test:")
    for i, prompt in enumerate(prompts):
        try:
            input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
            
            with torch.no_grad():
                generated = input_ids.clone()
                
                for _ in range(max_tokens):
                    if generated.size(1) >= model.max_seq_len:
                        break
                    
                    logits = model(generated)
                    next_token_logits = logits[:, -1, :] / 0.8  # Temperature
                    
                    # Top-p sampling
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                    
                    sorted_indices_to_remove = cumulative_probs > 0.9
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    
                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                    next_token_logits[indices_to_remove] = float('-inf')
                    
                    probs = F.softmax(next_token_logits, dim=-1)
                    next_token = torch.multinomial(probs, 1)
                    generated = torch.cat([generated, next_token], dim=1)
                    
                    if next_token.item() == tokenizer.eos_token_id:
                        break
            
            text = tokenizer.decode(generated[0], skip_special_tokens=True)
            print(f"  Prompt {i+1}: {text}")
        
        except Exception as e:
            print(f"  âŒ Error with prompt {i+1}: {e}")
    
    model.train()


def train_splatflow_on_real_data():
    """Train SplatFlow on real data to test if it works"""
    print("ğŸš€ SplatFlow Real Data Training")
    print("=" * 50)
    print("ğŸ¯ Goal: Test if SplatFlow can learn from real data")
    print()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")
    
    if device.type == 'cuda':
        gpu_props = torch.cuda.get_device_properties(0)
        gpu_memory = gpu_props.total_memory / 1024**3
        print(f"GPU: {gpu_props.name}")
        print(f"GPU Memory: {gpu_memory:.1f}GB")
        
        cleanup_memory()
        mem_info = get_gpu_memory_info()
        print(f"Available: {mem_info['free']:.2f}GB")
    
    # Initialize tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    tokenizer.pad_token = tokenizer.eos_token
    vocab_size = tokenizer.vocab_size
    
    # Configuration - Conservative but functional
    config = {
        'max_seq_len': 1024,         # Start smaller for stability
        'model_dim': 256,            # Good balance
        'num_layers': 4,             # Sufficient depth
        'num_splats': 16,            # Proven to work
        'batch_size': 4,             # Safe batch size
        'accumulation_steps': 4,     # Effective batch of 16
        'epochs': 20,                # Enough to see learning
        'dataset_size': 1000,        # Manageable size
        'learning_rate': 2e-4,       # Conservative
        'use_sparse_splats': True,   # Better efficiency
        'top_k_splats': 8,           
        'gradient_clip': 1.0,
        'weight_decay': 0.01
    }
    
    print(f"ğŸ“‹ Training Configuration:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    # Create dataset
    print(f"\nğŸ“š Creating Dataset...")
    dataset = RealDataset(
        tokenizer,
        seq_length=config['max_seq_len'],
        total_sequences=config['dataset_size']
    )
    
    if len(dataset) == 0:
        print("âŒ Failed to create dataset")
        return None
    
    dataloader = DataLoader(
        dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=0,
        pin_memory=False
    )
    
    print(f"âœ… Dataset ready: {len(dataset)} sequences")
    
    # Create model
    print(f"\nğŸ¤– Creating SplatFlow Model...")
    cleanup_memory()
    
    model = SplatFlowGPT(
        vocab_size=vocab_size,
        model_dim=config['model_dim'],
        num_layers=config['num_layers'],
        num_splats=config['num_splats'],
        max_seq_len=config['max_seq_len'],
        use_sparse_splats=config['use_sparse_splats'],
        top_k_splats=config['top_k_splats']
    ).to(device)
    
    # Training setup
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config['learning_rate'],
        weight_decay=config['weight_decay']
    )
    
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config['epochs']
    )
    
    # Test prompts
    test_prompts = [
        "Once upon a time",
        "The future of technology",
        "In a small village",
        "Scientists recently discovered"
    ]
    
    print(f"\nğŸ”¥ Starting Training ({config['epochs']} epochs)...")
    
    training_log = {'losses': [], 'epochs': []}
    start_time = time.time()
    
    for epoch in range(config['epochs']):
        epoch_start = time.time()
        model.train()
        
        epoch_loss = 0
        epoch_batches = 0
        
        print(f"\nEpoch {epoch + 1}/{config['epochs']}")
        print("-" * 30)
        
        optimizer.zero_grad()
        
        for batch_idx, batch in enumerate(dataloader):
            try:
                batch = batch.to(device, non_blocking=True)
                inputs = batch[:, :-1]
                targets = batch[:, 1:]
                
                # Forward pass
                logits = model(inputs)
                loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))
                
                # Scale for accumulation
                loss = loss / config['accumulation_steps']
                loss.backward()
                
                epoch_loss += loss.item() * config['accumulation_steps']
                
                # Update weights
                if (batch_idx + 1) % config['accumulation_steps'] == 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config['gradient_clip'])
                    optimizer.step()
                    optimizer.zero_grad()
                
                epoch_batches += 1
                
                if batch_idx % 20 == 0:
                    mem_info = get_gpu_memory_info()
                    print(f"  Batch {batch_idx+1:3d}/{len(dataloader)}: "
                          f"Loss={loss.item()*config['accumulation_steps']:.4f}, "
                          f"Mem={mem_info['allocated']:.2f}GB")
                
            except torch.cuda.OutOfMemoryError:
                print(f"âŒ OOM at batch {batch_idx}, skipping...")
                cleanup_memory()
                optimizer.zero_grad()
                continue
        
        # Epoch summary
        epoch_time = time.time() - epoch_start
        avg_loss = epoch_loss / epoch_batches if epoch_batches > 0 else float('inf')
        
        training_log['epochs'].append(epoch + 1)
        training_log['losses'].append(avg_loss)
        
        print(f"\nğŸ“Š Epoch {epoch + 1} Complete:")
        print(f"   Loss: {avg_loss:.4f}")
        print(f"   Time: {epoch_time:.1f}s")
        
        scheduler.step()
        
        # Test generation every 5 epochs
        if (epoch + 1) % 5 == 0:
            test_generation(model, tokenizer, test_prompts, device)
        
        cleanup_memory()
    
    total_time = time.time() - start_time
    
    print(f"\nğŸ Training Complete!")
    print(f"   Total Time: {total_time/60:.1f} minutes")
    print(f"   Final Loss: {training_log['losses'][-1]:.4f}")
    
    # Final generation test
    print(f"\nğŸ”¬ Final Generation Test:")
    test_generation(model, tokenizer, test_prompts, device, max_tokens=60)
    
    # Save model
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config,
        'training_log': training_log,
        'tokenizer_name': 'gpt2'
    }, 'splatflow_real_data.pt')
    
    print(f"ğŸ’¾ Model saved: splatflow_real_data.pt")
    
    return model, tokenizer, config, training_log


if __name__ == "__main__":
    print("ğŸš€ Testing SplatFlow on Real Data")
    print("Goal: See if SplatFlow can learn to generate coherent text")
    print()
    
    try:
        model, tokenizer, config, log = train_splatflow_on_real_data()
        
        if model is not None:
            print(f"\nğŸ‰ SUCCESS! SplatFlow trained on real data")
            print(f"âœ… Model learned to reduce loss from {log['losses'][0]:.4f} to {log['losses'][-1]:.4f}")
            print(f"âœ… Generated text samples above show learning progress")
            print(f"âœ… O(n*k) efficiency maintained throughout training")
    
    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
